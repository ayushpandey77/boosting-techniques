{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "\n",
        "=Boosting is an ensemble method that combines many weak learners (like small decision trees) to build a strong learner.\n",
        "\n",
        "How it works:\n",
        "\n",
        "-Train a weak learner.\n",
        "\n",
        "-Increase weights on misclassified samples.\n",
        "\n",
        "-Train the next learner focusing on difficult cases.\n",
        "\n",
        "-Repeat for many rounds.\n",
        "\n",
        "-Combine all learners (weighted vote/sum).\n",
        "\n",
        "Why it improves weak learners:\n",
        "\n",
        "-Each new learner fixes errors of the previous one.\n",
        "\n",
        "-Focuses more on hard-to-classify data.\n",
        "\n",
        "-Reduces bias and variance → better accuracy."
      ],
      "metadata": {
        "id": "Q9cqCVgNaxmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "\n",
        "=Difference between AdaBoost and Gradient Boosting\n",
        "\n",
        "AdaBoost (Adaptive Boosting):\n",
        "\n",
        "Trains models sequentially.\n",
        "\n",
        "Each new model is trained by increasing weights of the misclassified samples from the previous model.\n",
        "\n",
        "Focus = “harder” samples.\n",
        "\n",
        "Gradient Boosting:\n",
        "\n",
        "Trains models sequentially as well.\n",
        "\n",
        "Instead of weights, each new model is trained to predict the residual errors (gradients) of the previous model.\n",
        "\n",
        "Focus = reducing overall loss function."
      ],
      "metadata": {
        "id": "UstcjOYiaxjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: How does regularization help in XGBoost?\n",
        "\n",
        "=In XGBoost, regularization plays a key role in controlling the complexity of the model and preventing overfitting. It uses both L1 (Lasso) and L2 (Ridge) regularization as part of its objective function to penalize large or unnecessary weights in the trees. By applying this penalty, XGBoost discourages overly complex trees and reduces the chance of fitting noise in the training data. This leads to simpler, more stable models that generalize better to unseen data, ultimately improving accuracy and robustness."
      ],
      "metadata": {
        "id": "Qx_Eb-pQaxgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "=CatBoost is considered efficient for handling categorical data because it has a built-in mechanism to automatically process categorical features without requiring manual preprocessing like one-hot encoding or label encoding. It uses a technique called “ordered target statistics” and “permutation-driven encoding”, which convert categorical values into numerical representations based on target distribution while avoiding target leakage. This approach not only saves preprocessing time but also reduces the risk of overfitting that often occurs with traditional encodings. As a result, CatBoost handles categorical variables more efficiently and provides high accuracy with minimal feature engineering."
      ],
      "metadata": {
        "id": "gjXVCRZdaxeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "\n",
        "\n",
        "=Boosting techniques are often preferred over bagging methods in real-world applications where achieving high accuracy and handling complex patterns is more important than just reducing variance. For example, boosting is widely used in fraud detection, where rare and hard-to-detect patterns need extra focus, and in credit scoring to better predict loan defaults. It is also applied in customer churn prediction and recommendation systems, where small improvements in prediction accuracy can greatly impact business outcomes. In healthcare, boosting is used for disease prediction and medical diagnosis, as it can focus on difficult cases that bagging might miss. Overall, boosting is chosen in scenarios that demand precision and robustness, especially with imbalanced or noisy datasets."
      ],
      "metadata": {
        "id": "G66gV3Cqaxbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets:\n",
        "● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "● Use sklearn.datasets.fetch_california_housing() for regression\n",
        "tasks.\n",
        "\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "ex7El8gfaxZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: AdaBoost Classifier on Breast Cancer dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"AdaBoost Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrN8aui_b-WG",
        "outputId": "4363f86b-9a11-4a43-ef89-f3046d6cf043"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score\n"
      ],
      "metadata": {
        "id": "lbBJxlAxaxW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Gradient Boosting Regressor on California Housing dataset\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Gradient Boosting Regressor R² Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MV7twX3cR5G",
        "outputId": "3d7018a3-2fcc-4550-bd64-91c20c647333"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R² Score: 0.8004451261281281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "AuUgPTWcaxUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: XGBoost Classifier with GridSearchCV on Breast Cancer dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define XGBoost Classifier\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
        "\n",
        "# Define parameter grid for learning_rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Train model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"XGBoost Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4g8Cd56cdSA",
        "outputId": "3e3380bf-4289-490f-93b7-b704b3132c2a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "Best Parameters: {'learning_rate': 0.2}\n",
            "XGBoost Classifier Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [20:52:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "Axa7PnrFceXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "SL_FPQjoatGo",
        "outputId": "423fd362-aaa4-4843-da10-fe7632953621"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3980412067.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Question 9: CatBoost Classifier and Confusion Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_breast_cancer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Question 9: CatBoost Classifier and Confusion Matrix\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=200, learning_rate=0.1, depth=4, verbose=0, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"CatBoost Classifier Accuracy:\", accuracy)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot using seaborn\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Malignant','Benign'], yticklabels=['Malignant','Benign'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You're working for a FinTech company trying to predict loan default using\n",
        "\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model"
      ],
      "metadata": {
        "id": "KnEJRdgqck5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=To predict loan defaults in an imbalanced dataset with numeric and categorical features, a structured data science pipeline using boosting techniques would be as follows:\n",
        "\n",
        "1. Data Preprocessing: Start by handling missing values—impute numeric features using mean or median, and categorical features using the mode or most frequent value. For categorical variables, use target or one-hot encoding if using XGBoost or AdaBoost, while CatBoost can handle categorical features automatically. Address class imbalance with techniques like SMOTE, ADASYN, or class weighting to ensure the model effectively learns minority class patterns.\n",
        "\n",
        "2. Choice of Boosting Algorithm: Prefer XGBoost or CatBoost. XGBoost is robust for structured data and allows precise control of hyperparameters, while CatBoost is highly efficient with categorical data and reduces overfitting with ordered boosting. AdaBoost is less suitable for highly imbalanced datasets.\n",
        "\n",
        "3. Hyperparameter Tuning: Use GridSearchCV or RandomizedSearchCV to tune key parameters such as learning_rate, n_estimators, max_depth, and regularization terms (reg_alpha, reg_lambda) to balance bias and variance and improve generalization.\n",
        "\n",
        "4. Evaluation Metrics: Since the dataset is imbalanced, rely on ROC-AUC, precision, recall, and F1-score rather than accuracy. These metrics ensure the model accurately identifies potential defaulters and minimizes false negatives, which are costly for financial institutions.\n",
        "\n",
        "5. Business Impact: The model enables the company to identify high-risk borrowers before issuing loans, reducing financial losses and improving credit portfolio quality. Accurate predictions support better decision-making, optimize lending strategies, and enhance profitability while maintaining customer trust."
      ],
      "metadata": {
        "id": "fYdPoEyRc1RW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dssdJGSGcqP-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}